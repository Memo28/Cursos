# -*- coding: utf-8 -*-
"""ScrappyPagina12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KOJMJmNahHhYHnbAZRi1YxQ226q2_5v6
"""

!pip install scrapy

import scrapy 
from scrapy.crawler import CrawlerProcess

class Spider12(scrapy.Spider):
  name = 'spider12'
  # Definimos que dominios vamos a scrapear
  allowed_domains = ['pagina12.com.ar']
  custom_settings = {'FEED_FORMAT': 'json', 'FEED_URI': 'resultados.json', 'DEEPTH_LIMIT' : 2}


  start_urls = ['https://www.pagina12.com.ar/secciones/el-pais',
 'https://www.pagina12.com.ar/secciones/economia',
 'https://www.pagina12.com.ar/secciones/sociedad',
 'https://www.pagina12.com.ar/suplementos/cultura-y-espectaculos',
 'https://www.pagina12.com.ar/secciones/el-mundo',
 'https://www.pagina12.com.ar/secciones/deportes',
 'https://www.pagina12.com.ar/secciones/plastica',
 'https://www.pagina12.com.ar/secciones/contratapa']
   
  
  def parse(self, response):
    # Encontramos el articulo promocionado
    nota_promocionada = response.xpath('//div[@class="featured-article__container"]/h2/a/@href').get()
    if nota_promocionada is not None:
      # response.follow lo que hace es que hace un request al link y con un callback
      # lleva la respuesta hacia el metodo parse_nota
      yield response.follow(nota_promocionada, callback=self.parse_nota)

    notas = response.xpath('//ul[@class="article-list"//li//a//@href]').getall()
    for nota in notas:
      yield response.follow(nota, callback=self.parse_nota)

    # Link a la siguiente pagina
    nex_page = response.xpath('//a[@class="pagination-btn-next"]/@href')
    if next_page is not None:
      yield response.follow(next_page, callback=self.parse)

  def parse_nota(self, response):
    # Parsep de la nota
    titulo = response.xpath('//div[@class="article-title"]/text()').get()
    cuerpo= ''.join(response.xpath('//div[@class="article-text"]/p/text()').getall())

    yield {'url': response.url, 'titulo': titulo, 'cuerpo': cuerpo}

process = CrawlerProcess()
process.crawl(Spider12)
process.start()

[]